{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pytube import YouTube\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment, silence\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from urllib.request import urlopen, Request\n",
    "import spacy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize a speech recognition recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Define a function to download audio from a YouTube video URL and transcribe it\n",
    "def transcribe_audio(youtube_url):\n",
    "    try:\n",
    "        # Download the YouTube video\n",
    "        youtube = YouTube(youtube_url)\n",
    "\n",
    "        # Get the audio stream\n",
    "        audio_stream = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "        # Download the audio\n",
    "        audio_file = audio_stream.download()\n",
    "\n",
    "        # Convert the audio to WAV format using PyDub\n",
    "        audio = AudioSegment.from_file(audio_file)\n",
    "        wav_file = audio_file.replace(\".webm\", \".wav\")  # Change the extension\n",
    "        audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "        # Load the audio file for recognition\n",
    "        with sr.AudioFile(wav_file) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "\n",
    "        return audio_data, wav_file  # Return both the audio data and the WAV file path\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {str(e)}\")\n",
    "        return None, \"\"\n",
    "\n",
    "# Function to split audio based on silence threshold\n",
    "def split_audio_on_silence(audio, silence_thresh=35):\n",
    "    audio_parts = silence.split_on_silence(audio, silence_thresh=silence_thresh)\n",
    "    return audio_parts\n",
    "\n",
    "# Function to process player data\n",
    "def process_player_data(player_data):\n",
    "    playerName, youtube_url = player_data\n",
    "\n",
    "    # Transcribe the audio from the YouTube video\n",
    "    audio_data, wav_file = transcribe_audio(youtube_url)\n",
    "\n",
    "    if audio_data is None:\n",
    "        return None\n",
    "\n",
    "    # Load the audio file for silence detection\n",
    "    audio = AudioSegment.from_file(wav_file)\n",
    "\n",
    "    # Split the audio based on silence\n",
    "    audio_parts = split_audio_on_silence(audio)\n",
    "\n",
    "    # Concatenate the audio parts to get rid of silence\n",
    "    concatenated_audio = AudioSegment.silent()\n",
    "    for part in audio_parts:\n",
    "        concatenated_audio += part\n",
    "\n",
    "    # Perform speech recognition on the concatenated audio\n",
    "    try:\n",
    "        print(f\"Processing data for {playerName}...\")\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "\n",
    "        # Use spaCy for sentence segmentation\n",
    "        doc = nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        # Separate sentences into questions and answers\n",
    "        questions = sentences[::2]\n",
    "        answers = sentences[1::2]\n",
    "\n",
    "        # Check if array sizes are different and remove the last question if needed\n",
    "        if len(questions) > len(answers):\n",
    "            questions.pop()\n",
    "\n",
    "        # Create a player dataframe\n",
    "        player_df = pd.DataFrame({\n",
    "            'PlayerName': [playerName] * len(answers),\n",
    "            'Questions': questions,\n",
    "            'Answers': answers,\n",
    "        })\n",
    "\n",
    "        return player_df\n",
    "    except sr.UnknownValueError:\n",
    "        print(f\"Google Speech Recognition could not understand audio for {playerName}. Skipping...\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service for {playerName}; {str(e)}. Skipping...\")\n",
    "        return None\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file = \"interviews.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Read the CSV file containing player stats (player_name, games_played)\n",
    "url = \"https://www.hockeydb.com/ihdb/draft/nhl2016e.html\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "req = Request(url, headers=headers)\n",
    "html = urlopen(req)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract headers from the first row\n",
    "headers = [th.text.strip() for th in table.find_all('tr')[1].find_all('th')]\n",
    "\n",
    "# Create empty lists to store the table data\n",
    "data = []\n",
    "\n",
    "# Extract the table rows (skip the first row as it contains headers)\n",
    "for row in table.find_all('tr')[2:]:\n",
    "    row_data = [cell.text.strip() for cell in row.find_all('td')]\n",
    "    data.append(row_data)\n",
    "\n",
    "# Create the player_stats DataFrame\n",
    "player_stats = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Create a list of player data to process\n",
    "player_data_list = [(row['playerName'], row['url']) for _, row in df.iterrows()]\n",
    "\n",
    "# Function to train the machine learning model\n",
    "def train_model(X_train, y_train):\n",
    "    # Define a text processing pipeline using TF-IDF vectorization and linear regression\n",
    "    text_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),  # Adjust max_features as needed\n",
    "        ('model', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    text_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    return text_pipeline\n",
    "\n",
    "# Function to process player data and return relevant information\n",
    "def process_player_data(player_data):\n",
    "    playerName, youtube_url = player_data\n",
    "\n",
    "    # Transcribe the audio from the YouTube video\n",
    "    audio_data, wav_file = transcribe_audio(youtube_url)\n",
    "\n",
    "    if audio_data is None:\n",
    "        return None\n",
    "\n",
    "    # Load the audio file for silence detection\n",
    "    audio = AudioSegment.from_file(wav_file)\n",
    "\n",
    "    # Split the audio based on silence\n",
    "    audio_parts = split_audio_on_silence(audio)\n",
    "\n",
    "    # Concatenate the audio parts to get rid of silence\n",
    "    concatenated_audio = AudioSegment.silent()\n",
    "    for part in audio_parts:\n",
    "        concatenated_audio += part\n",
    "\n",
    "    # Perform speech recognition on the concatenated audio\n",
    "    try:\n",
    "        print(f\"Processing data for {playerName}...\")\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "\n",
    "        # Use spaCy for sentence segmentation\n",
    "        doc = nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        # Separate sentences into questions and answers\n",
    "        questions = sentences[::2]\n",
    "        answers = sentences[1::2]\n",
    "\n",
    "        # Check if array sizes are different and remove the last question if needed\n",
    "        if len(questions) > len(answers):\n",
    "            questions.pop()\n",
    "\n",
    "        # Create a player dataframe\n",
    "        player_df = pd.DataFrame({\n",
    "            'PlayerName': [playerName] * len(answers),\n",
    "            'Questions': questions,\n",
    "            'Answers': answers,\n",
    "        })\n",
    "\n",
    "        return player_df\n",
    "    except sr.UnknownValueError:\n",
    "        print(f\"Google Speech Recognition could not understand audio for {playerName}. Skipping...\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service for {playerName}; {str(e)}. Skipping...\")\n",
    "        return None\n",
    "\n",
    "# Process player data concurrently\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    processed_results = list(executor.map(process_player_data, player_data_list))\n",
    "\n",
    "# Filter out None results\n",
    "processed_results = [result for result in processed_results if result is not None]\n",
    "\n",
    "# Concatenate player dataframes into a final dataframe if there are any\n",
    "if processed_results:\n",
    "    final_df = pd.concat(processed_results, ignore_index=True)\n",
    "\n",
    "    # Merge with player_stats DataFrame on player name\n",
    "    merged_df = pd.merge(final_df, player_stats, left_on='PlayerName', right_on='Player', how='inner')\n",
    "\n",
    "    # Define features (answers) and target (games_played)\n",
    "    X = merged_df['Answers']\n",
    "    y = merged_df['GP'].astype(int)  # Convert games played to integer\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the machine learning model\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "    # Display the top 5 words that correlate with 'GP'\n",
    "    feature_names = model.named_steps['tfidf'].get_feature_names_out()\n",
    "    coef = model.named_steps['model'].coef_\n",
    "    top_5_words = [feature_names[i] for i in coef.argsort()[-5:][::-1]]\n",
    "    print(f'Top 5 words that correlate with GP: {top_5_words}')\n",
    "\n",
    "    # Display the final dataframe\n",
    "    print(merged_df)\n",
    "else:\n",
    "    print(\"No valid data to concatenate. Check the speech recognition process.\")\n",
    "player_stats.to_csv('drafted_players.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
